\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
% \setlength{\parskip}{\baselineskip}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{amsmath,amssymb,amsrefs}
\usepackage[top=24mm, bottom=18mm, left=15mm, right=13mm]{geometry}
\usepackage{url}

\begin{document}

\begin{flushright}
Lawrence Ouyang\\
504128219\\
Math 191\\
Homework 2\\
\end{flushright}

1a.
\begin{align} \nonumber
f_X(x) &=  
  \begin{cases}
  \begin{aligned}
  & \frac{1}{2}, &  -1 \leq x \leq 1 \\ \\
  & 0, & x < -1, x > 1
  \end{aligned}
  \end{cases} \\ \nonumber \\ \nonumber
f_Y(y) &=  
    \begin{cases}
    \begin{aligned}
    & 1, &  0 \leq y \leq 1 \\ \\
    & 0, & y < 0, y > 1
    \end{aligned}
    \end{cases} \\ \nonumber \\ \nonumber
f_{Y|X}(y|x) &= 
  \begin{cases}
  \begin{aligned}
  & 1, & -1 \leq x < 0 \\
  & 1, & 0 < x \leq 1
  \end{aligned}
  \end{cases} \\ \nonumber \\ \nonumber
f_{X,Y}(x,y) &= f_{Y|X}(y|x)f_X(x) = f_X(x)f_Y(y)
\end{align}
\begin{center}
Thus X and Y are independent and their correlation is 0.
\end{center}
1b. Since $\alpha$ is distributed uniformly $ \alpha \sim \mbox{Unif}([0 , 2 \pi])$, then X and Y are uniformly distributed on $[-1,1]$ and $\mathbb{E}[X] = \mathbb{E}[Y] = 0$. Thus $cov(X,Y) = cor(X,Y) = 0$ which means X and Y are uncorrelated. However, by trigonometric identities, we notice that for all $x \in X, y \in Y = \pm\sqrt{1-x^2}$, thus Y is dependent of X. \\

2a. The closed form for $\hat{\beta_0}$ is $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$. We denote $\bar{y} = \frac{1}{n}\sum_{i=1}^{n}\beta_0 + \beta_1x_i + e_i$ and $\bar{x} = \sum_{i = 1}^{n}\frac{x_i}{n}$:
\begin{align} \nonumber
\mathbb{E}[\hat{\beta_0}] &= \mathbb{E}[\bar{y}] - \bar{x}\mathbb{E}[\hat{\beta_1}] \\ \nonumber
&= \mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}\beta_0 + \beta_1x_i + e_i] - \bar{x}\hat{\beta_1} \\ \nonumber
&= \beta_0 + \hat{\beta_1}\bar{x} - \bar{x}\hat{\beta_1} \\ \nonumber
&= \beta_0
\end{align}

2b. Consider the OLS Estimator $\hat{\beta}$ to have the form $(X^TX)^{-1}X^Ty$, where $y = X\beta + \mu$ and $\mu$ is the residual. 
\begin{align} \nonumber
\mathbb{E}[\hat{\beta}] &= \mathbb{E}[(X^TX)^{-1}X^Ty] \\ \nonumber
&= \mathbb{E}[(X^TX)^{-1}X^T(X\beta+\mu)] \\ \nonumber
&= \mathbb{E}[(X^TX)^{-1}X^TX\beta +(X^TX)^{-1}X^T\mu] \\ \nonumber
&= \mathbb{E}[\beta + (X^TX)^{-1}X^T\mu] \\ \nonumber
&= \beta + \mathbb{E}[(X^TX)^{-1}X^T\mu], \mathbb{E}[\mu] = 0\\ \nonumber
&= \beta 
\end{align}

3. Use the same notations from 2a. 
\begin{align} \nonumber
var(\hat{\beta_0}) &= var(\bar{y} - \hat{\beta_1}\bar{x}) \\ \nonumber
&= var(\bar{y}) - \bar{x}^2var(\hat{\beta_1}) - 2cov(\bar{y},\bar{x}\hat{\beta_1}) \\ \nonumber
&= \frac{var(y_i)}{n} -  \bar{x}^2\frac{var(y_i)}{\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2} -2\bar{x}^2cov(\bar{y},\hat{\beta_1}), var(y_i) = \sigma^2 \\ \nonumber
&= \frac{\sigma^2}{n} -\frac{\bar{x}^2\sigma^2}{\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2}-2\bar{x}^2cov(\bar{y},\hat{\beta_1}) \\ \nonumber
cov(\bar{y},\hat{\beta_1}) &= \frac{\sum_{i=1}^{n}(x_i - \bar{x})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}cov(\bar{y},y_i) \\ \nonumber
&= 0 \\ \nonumber
var(\hat{\beta_0})&= \frac{\sigma^2}{n} -\frac{\bar{x}^2\sigma^2}{\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2}\\ \nonumber
&= \sigma^2\frac{\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2 + n\bar{x}^2}{n\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2} \\ \nonumber
&= \sigma^2\frac{\sum_{i = 1}^{n}x_i^2}{n\sum_{i = 1}^{n}x_i^2 - n\bar{x}^2} 
\end{align}

4.
\[ \phi_X(t) = 
\begin{cases}
  \begin{aligned}
  &  \mathbb{E}[e^{itX}], &  \mbox{ if } x \geq 0 \\
  & 0, & \mbox{if } x < 0
  \end{aligned}
\end{cases}
\]
\begin{align} \nonumber
\phi_X(t) &= \mathbb{E}[e^{itX}] \\ \nonumber
&= \int_{0}^{\infty}\lambda e^{-\lambda x}e^{itx}dx \\ \nonumber
&= \lambda\int_{0}^{\infty}e^{x(it - \lambda)}dx \\ \nonumber
&= \frac{\lambda}{it - \lambda}e^{x(it-\lambda)}\big|_0^\infty \\ \nonumber
&= \frac{\lambda}{it - \lambda}
\end{align}

5.
\begin{align} \nonumber
H(X,Y) &= -\sum_x\sum_yp(x,y)logp(x,y) \\ \nonumber
&= -\sum_x\sum_yp(x,y)log(p(x)p(y|x)) \\ \nonumber
&= -\sum_x\sum_yp(x,y)logp(x) -\sum_x\sum_yp(x,y)logp(y|x) \\ \nonumber
&= -\sum_xp(x)logp(x) -\sum_x\sum_yp(x,y)logp(y|x) \\ \nonumber
&= H(X) + H(Y|X)
\end{align}

6a. Marginal distributions: \\
\begin{align} \nonumber
p_x(X) &= \{\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}\} \\ \nonumber
p_y(Y) &= \{\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\} \\ \nonumber
\end{align}

6b. Entropy: \\
\begin{align} \nonumber
H(X) &= -\frac{1}{2}log\frac{1}{2} -\frac{1}{4}log\frac{1}{4} -2\frac{1}{8}log\frac{1}{8} = \frac{7}{4} \\ \nonumber
H(Y) &= -log\frac{1}{4} = 2
\end{align}

6c. Joint/Conditional Entropy: \\
\begin{align} \nonumber
H(X|Y) &= \sum_{i=1}^4p(Y=i)H(X|Y=i) \\ \nonumber
&= \frac{1}{4}(H(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8})+H(\frac{1}{4}, \frac{1}{2}, \frac{1}{8}, \frac{1}{8})) + H(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}) + H(1, 0, 0, 0)) \\ \nonumber
&= \frac{1}{4}(\frac{7}{4}+\frac{7}{4} + 2) = \frac{11}{8} \\ \nonumber \\ \nonumber
H(Y|X) &= \sum_{i=1}^4p(X=i)H(Y|X=i) \\ \nonumber
&= \frac{1}{2}H(\frac{1}{4}, \frac{1}{8}, \frac{1}{8}, \frac{1}{2}) + \frac{1}{4}H(\frac{1}{4}, \frac{1}{2}, \frac{1}{4}, 0) + \frac{1}{4}H(\frac{1}{4}, \frac{1}{4}, \frac{1}{2}, 0) \\ \nonumber
&= \frac{1}{2}(\frac{7}{4}) + \frac{1}{2}(\frac{3}{2}) = \frac{13}{8} \\ \nonumber \\ \nonumber
H(X,Y) &= H(X) + H(Y|X) \\ \nonumber
&= \frac{7}{4} + \frac{13}{8} = \frac{27}{8}
\end{align}

7a. Consider mutual information to be defined as:
\begin{align} \nonumber
I(X;Y) &= \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)} \\ \nonumber
&= \sum_x\sum_yp(x,y)log\frac{p(x|y)}{p(x)} \\ \nonumber
&= \sum_x\sum_yp(x,y)logp(x|y) - \sum_x\sum_yp(x,y)logp(x) \\ \nonumber
&= -H(X|Y) + H(X) \\ \nonumber
&= H(X) - H(X|Y)
\end{align}

\newpage

7b. 
\begin{align} \nonumber
I(X;Y) &= \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)} \\ \nonumber
&= \sum_x\sum_yp(x,y)logp(x,y) - \sum_x\sum_yp(x,y)logp(x) - \sum_x\sum_yp(x,y)logp(y) \\ \nonumber
&= \sum_x\sum_yp(x,y)logp(x,y) - \sum_xp(x)logp(x) - \sum_yp(y)logp(y) \\ \nonumber
&= -H(X,Y) + H(X) + H(Y) \\ \nonumber
&= H(X) + H(Y) - H(X,Y)
\end{align}

7c.
\begin{align} \nonumber
I(X;Y) &= \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)} \\ \nonumber
&= \sum_x\sum_yp(x,y)log\frac{p(y|x)}{p(y)} \\ \nonumber
&= \sum_x\sum_yp(x,y)logp(y|x) - \sum_x\sum_yp(x,y)logp(y) \\ \nonumber
&= - H(Y|X) + H(Y) \\ \nonumber
&= H(Y) - H(Y|X) \\ \nonumber
&= I(Y;X)
\end{align}

7d.
\begin{align} \nonumber
I(X;X) &= \sum_x\sum_xp(x,x)log\frac{p(x,x)}{p(x)p(x)} \\ \nonumber
&= \sum_xp(x)log\frac{p(x)}{p(x)} - \sum_xp(x)logp(x)\\ \nonumber
&= -\sum_xp(x)logp(x) \\ \nonumber
&= H(X)
\end{align}

\newpage

8ai. Yes, as the predictor(horsepower) increases, the response(mpg) decreases. The coefficients given for this simple linear regression model is 39.935861 for the intercept and -0.157845 for the slope.
\\ \\
8aii. The relationship is somewhat strong with an adjusted R-squared score of 0.6049. The near zero p-value does indicate a strong rejection of the null.
\\ \\
8aiii. The relationship is negative: as the predictor increases, the response decreases.
\\ \\
8aiv. The predicted mpg associated with 98 horsepower is 24.46708, with a confidence interval [23.97308, 24.96108] and prediction interval [14.8094, 34.12476].
\\ \\
8b, 8c. See attached plots.
\\ \\
9. Programming Assignment
\\
(a). Using the order \{SPY, XVIX, TNX, OIL, GOLD, N225, FTSE, GLD\}, our $\beta$ values are:
\begin{multicols}{2}
\begin{align} \nonumber
\beta_0 &= 0.0004742 \\ \nonumber
\beta_1 &= -0.1576429 \\ \nonumber
\beta_2 &= -0.0067277 \\ \nonumber
\beta_3 &= 0.0129053 \\ \nonumber
\beta_4 &= 0.0030628
\end{align}
\columnbreak
\begin{align} \nonumber
\\ \nonumber
\beta_5 &= 0.0003391 \\ \nonumber
\beta_6 &= 0.0106196 \\ \nonumber
\beta_7 &= 0.0930072 \\ \nonumber
\beta_8 &= 0.0409372
\end{align}
\end{multicols}
\begin{align} \nonumber
\hat{r}_{t+1,SPY} &= 0.0004742 - 0.1576429r_{t,SPY} - 0.0067277r_{t,XVIX} + 0.0129053r_{t,TNX} + 0.0030628r_{t,OIL} \\ \nonumber
 &+ 0.0003391r_{t,GOLD} + 0.0106196r_{t,N225} + 0.0930072_{t,FTSE} + 0.0409372_{t,GLD}
\end{align}
\\
(b). Here is the code for part b:
\begin{lstlisting}
doWork_In_Sample = 1
if(doWork_In_Sample == 1){

	X_train = RET[1:nrDays - 1,];
	X_test = RET;
	STATS = NULL;
	CM = NULL;
	for (i in 1:nrTickers) {
		print('#############################################');
		print( paste('Stock = ',tickers[i] ) );
		y_train = RET[ 2 : nrDays , tickers[i], drop=FALSE];  # "drop=FALSE" ensurs it remains a 2-D array..
	
		y_hat = compute_linear_regression(X_train, y_train, X_test);
		# compute the various performance statistics:
		
		daily_pnl = matrix(, nrow = nrDays - 1, ncol = 1);
		for (j in 1:(nrDays - 1)) {
			if (y_hat[j] < 0) {
				daily_pnl[j] = -RET[j+1,i]
			}
			else {
				daily_pnl[j] = RET[j+1,i];
			}
		}
		cum_pnl = cumsum(daily_pnl);
		CM = cbind(CM,cum_pnl);
		mean_pnl = mean(daily_pnl, na.rm=TRUE);
		yearly_pnl = mean(daily_pnl, na.rm=TRUE) * 252;
		total_pnl = sum(daily_pnl, na.rm = TRUE);
		sharpe = compute_Sharpe_Ratio(daily_pnl);
		
		stats_this_stock = c(sharpe, mean_pnl, yearly_pnl, total_pnl)
		STATS = rbind(STATS,stats_this_stock);

		readline('Press key to continue...');
	}

	rownames(STATS) = tickers;
	colnames(STATS) = c('sharpe', 'mean_pnl', 'yearly_pnl', 'total_pnl');;
	print(STATS);
	colnames(CM) = tickers;
	plot_cumsum(CM);

return('DONE!');
}
\end{lstlisting}
\hfill \break
(c). \\
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Stock Name & Sharpe & $\mu_{PNL}$ & $Ret_{ann}$ & $Ret_{total}$ \\
\hline
SPY & 0.98 & $6.2 \times 10^{-4}$ & 0.156 & 0.779 \\
\hline
VIX & 0.83 & $3.9 \times 10^{-3}$ & 0.986 & 4.938 \\
\hline
TNX & 0.90 & $1.3 \times 10^{-3}$ & 0.326 & 1.633 \\
\hline
OIL & 1.26 & $1.5 \times 10^{-3}$ & 0.379 & 1.896 \\
\hline
GOLD & 0.92 & $1.3 \times 10^{-3}$ & 0.336 & 1.682 \\
\hline
N225 & 6.18 & $5.0 \times 10^{-3}$ & 1.255 & 6.285 \\
\hline
FTSE & 2.61 & $1.6 \times 10^{-3}$ & 0.408 & 2.042 \\
\hline
GLD & 0.30 & $2.0 \times 10^{-4}$ & 0.052 & 0.258 \\
\hline
\end{tabular}
\end{center}
\newpage
(d). Here is the code for part d:
\begin{lstlisting}
doWork_In_Sample = 1
if(doWork_In_Sample == 1){

	X_train = RET[1:nrDays - 1,];
	X_test = RET;
	STATS = NULL;
	CM = NULL;

	for (i in 1:nrTickers) {
		#print('#############################################');
		#print( paste('Stock = ',tickers[i] ) );

		daily_pnl = matrix(, nrow = nrDays - 101, ncol = 1);

		for (j in 101: (nrDays - 1)) {
			y_train = RET[ (j - 99) : j , tickers[i], drop=FALSE];
			X_train = RET[ (j - 100) : (j - 1) ,]
			y_hat = compute_linear_regression(X_train, y_train, X_test);
			if(y_hat[j] < 0) {
				daily_pnl[j - 100] = -RET[j + 1, i];
			}
			else {
				daily_pnl[j - 100] = RET[j + 1, i];
			}
		}

		cum_pnl = cumsum(daily_pnl);
		CM = cbind(CM,cum_pnl);
		mean_pnl = mean(daily_pnl, na.rm=TRUE);
		yearly_pnl = mean(daily_pnl, na.rm=TRUE) * 252;
		total_pnl = sum(daily_pnl, na.rm = TRUE);
		sharpe = compute_Sharpe_Ratio(daily_pnl);
		stats_this_stock = c(sharpe, mean_pnl, yearly_pnl, total_pnl)
		STATS = rbind(STATS,stats_this_stock);
	}
	
	rownames(STATS) = tickers;
	colnames(STATS) = c('sharpe', 'mean_pnl', 'yearly_pnl', 'total_pnl');
	print(STATS);
	colnames(CM) = tickers;
	plot_cumsum(CM);
}

return('DONE!');
}
\end{lstlisting}
\newpage
(e). \\
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Stock Name & Sharpe & $\mu_{PNL}$ & $Ret_{ann}$ & $Ret_{total}$ \\
\hline
SPY & 0.26 & $1.6 \times 10^{-4}$ & 0.040 & 0.186 \\
\hline
VIX & 0.11 & $5.2 \times 10^{-4}$ & 0.131 & 0.605 \\
\hline
TNX & 0.55 & $8.1 \times 10^{-4}$ & 0.205 & 0.946 \\
\hline
OIL & $-0.40$ & $-4.8 \times 10^{-4}$ & $-0.121$ & $-0.556$ \\
\hline
GOLD & $-0.25$ & $-3.6 \times 10^{-4}$ & $-0.091$ & $-0.418$ \\
\hline
N225 & 5.58 & $4.5 \times 10^{-3}$ & 1.145 & 5.279 \\
\hline
FTSE & 1.47 & $9.0 \times 10^{-4}$ & 0.227 & 1.048 \\
\hline
GLD & 0.15 & $1.0 \times 10^{-4}$ & 0.025 & 0.117 \\
\hline
\end{tabular}
\end{center}
\hfill \break
We have much lower values in (e) than (c), which denote a much less profitable prediction with our second model. This makes sense since we had much less data to work with our sliding window approach, but that method may be a better prediction model. The model in (c) may over-fit, which would result in the very high Sharpe ratios and returns.
\end{document}